{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adam optimizer...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1600, 1])) that is different to the input size (torch.Size([64, 1, 5, 5])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 119\u001b[39m\n\u001b[32m    115\u001b[39m fake_labels = torch.zeros_like(output_real, device=device)\n\u001b[32m    118\u001b[39m output_real = disc(real_images)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m loss_real = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m noise = torch.randn(batch_size, LATENT_DIM, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, device=device)\n\u001b[32m    122\u001b[39m fake_images = gen(noise)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\loss.py:699\u001b[39m, in \u001b[36mBCELoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\functional.py:3560\u001b[39m, in \u001b[36mbinary_cross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction)\u001b[39m\n\u001b[32m   3558\u001b[39m     reduction_enum = _Reduction.get_enum(reduction)\n\u001b[32m   3559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target.size() != \u001b[38;5;28minput\u001b[39m.size():\n\u001b[32m-> \u001b[39m\u001b[32m3560\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3561\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is deprecated. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3562\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure they have the same size.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3563\u001b[39m     )\n\u001b[32m   3565\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3566\u001b[39m     new_size = _infer_size(target.size(), weight.size())\n",
      "\u001b[31mValueError\u001b[39m: Using a target size (torch.Size([1600, 1])) that is different to the input size (torch.Size([64, 1, 5, 5])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_fid import fid_score\n",
    "from PIL import Image\n",
    "\n",
    "# Set parameters\n",
    "IMG_SIZE = 64\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LATENT_DIM = 100\n",
    "EPOCHS = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocess dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image  # No labels, just images\n",
    "\n",
    "# Load Dataset\n",
    "dataset = ImageDataset(root_dir='images', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(LATENT_DIM, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# Define Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(-1, 1)  # Ensures output shape is (batch_size, 1)\n",
    "\n",
    "# Initialize models\n",
    "gen = Generator().to(device)\n",
    "disc = Discriminator().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizers to test\n",
    "optimizers = {\n",
    "    'Adam': optim.Adam,\n",
    "    'RMSprop': optim.RMSprop,\n",
    "    'SGD': optim.SGD\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for opt_name, opt_class in optimizers.items():\n",
    "    print(f\"Training with {opt_name} optimizer...\")\n",
    "    optimizer_g = opt_class(gen.parameters(), lr=0.0002)\n",
    "    optimizer_d = opt_class(disc.parameters(), lr=0.0002)\n",
    "    \n",
    "    loss_g_list, loss_d_list = [], []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, real_images in enumerate(dataloader):\n",
    "            real_images = real_images.to(device)\n",
    "            batch_size = real_images.size(0)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            optimizer_d.zero_grad()\n",
    "            output_real = disc(real_images)\n",
    "            real_labels = torch.ones_like(output_real, device=device)\n",
    "            fake_labels = torch.zeros_like(output_real, device=device)\n",
    "\n",
    "            loss_real = criterion(output_real, real_labels)\n",
    "            \n",
    "            noise = torch.randn(batch_size, LATENT_DIM, 1, 1, device=device)\n",
    "            fake_images = gen(noise)\n",
    "            output_fake = disc(fake_images.detach())\n",
    "            loss_fake = criterion(output_fake, fake_labels)\n",
    "            \n",
    "            loss_d = loss_real + loss_fake\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            optimizer_g.zero_grad()\n",
    "            output_fake = disc(fake_images)\n",
    "            loss_g = criterion(output_fake, real_labels)\n",
    "            loss_g.backward()\n",
    "            optimizer_g.step()\n",
    "            \n",
    "            loss_g_list.append(loss_g.item())\n",
    "            loss_d_list.append(loss_d.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}: Loss_D={loss_d.item():.4f}, Loss_G={loss_g.item():.4f}\")\n",
    "    \n",
    "    # Save loss results\n",
    "    results[opt_name] = {\n",
    "        'Generator Loss': loss_g_list,\n",
    "        'Discriminator Loss': loss_d_list\n",
    "    }\n",
    "\n",
    "# Compute FID Score for evaluation\n",
    "fid_values = {}\n",
    "for opt_name in optimizers.keys():\n",
    "    fake_images = gen(torch.randn(500, LATENT_DIM, 1, 1, device=device)).detach().cpu()\n",
    "    fake_images_path = f'fake_images_{opt_name}/'\n",
    "    os.makedirs(fake_images_path, exist_ok=True)\n",
    "    for i, img in enumerate(fake_images):\n",
    "        vutils.save_image(img, os.path.join(fake_images_path, f'{i}.png'), normalize=True)\n",
    "    \n",
    "    fid = fid_score.calculate_fid_given_paths(['images/', fake_images_path], batch_size=50, device=device, dims=192)\n",
    "    fid_values[opt_name] = fid\n",
    "\n",
    "print(\"FID Scores:\", fid_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
